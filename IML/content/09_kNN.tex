
	\section{Si descriva in modo accurato l’algoritmo k-NN, illustrando il ruolo dei principali iperparametri, i vantaggi e le debolezze del modello nei 
		confronti di altri algoritmi affrontati nel corso, e si evidenzi il principale inductive bias di tale algoritmo}
	
   L'algoritmo k-NN è un modello non parametrico, ovvero quel tipo di modello che
   richiede come parametri solamente i dati in input, a differenza di altri modelli come le reti neurali
   oppure la logistic regression\\
	È un \textit{lazy learner} poiché non costruisce un modello esplicito durante il training,
   ma posticipa tutta la computazione alla fase di prediction\\
	Questo tipo di modello è usato per approssimare una funzione target\\
	Nella fase di learning semplicemente si memorizzano i dati, invece, nella fase di prediction\\
	\textbf{Fase di prediction}: Per un nuovo punto $x_{\text{test}}$:
	\begin{enumerate}
		\item Calcola le distanze tra $x_{\text{test}}$ e tutti gli $x_i \in \mathcal{D}$
		\item Seleziona i $k$ punti più vicini ($\mathcal{N}_k$)
		\item Assegna la classe più frequente in $\mathcal{N}_k$ (moda)
	\end{enumerate}
	L'\textbf{inductive bias} è il \textbf{principio di località}:\\
	punti vicini nello spazio delle feature hanno etichette simili\\
	Formalmente:
	\[
	\forall x_i, x_j \in \mathcal{X}, \quad d(x_i,x_j) \approx 0 \implies P(y_i=y_j) \approx 1
	\]
	L'idea chiave è che il valore della funzione target per un sample è stimato dagli esempi di
   training memorizzati
	\begin{itemize}
		\item Norma 2 (Euclidea): $\sqrt{\sum_{i=1}^n (x_i-y_i)^2}$
		\item Norma 1 (Manhattan): $\sum_{i=1}^n |x_i-y_i|$
	\end{itemize}
	Il decision boundary viene descritto tramite il diagramma di Voronoi che mostra come gli spazi di
   input sono suddivisi fra le classi nel piano d-dimensionale\\
	Formalmente, il \textbf{diagramma di Voronoi} partiziona lo spazio in celle dove:
	\[
	V(x_i) = \{x \in \mathcal{X} \mid d(x,x_i) \leq d(x,x_j), \forall j \neq i\}
	\]
	\subsection{Vantaggi e svantaggi}
	Il principale vantaggio è che la fase di learning ha un tempo di O(1), ovvero richiede solo la
   memorizzazione dei dati, cosa non presente in diversi algoritmi e che può essere utilizzato per
   classificare le istanze di test fra più classi (a differenza ad esempio della logistic regression)\\
	Il principale svantaggio è la complessità, per la prediction infatti si ha una complessità di tipo
   $O(mn)$ (brute-force), riducibile a $O(n \log n)$ con strutture dati specializzate, ovvero
   una struttura ad albero per scendere più velocemente in base alla distanza\\
	Inoltre, altri problemi sono il fatto che alcune feature hanno range più larghi, quindi
   devono essere considerate più importanti (feature scaling), poi alcune feature possono
   essere anche irrilevanti (feature selection) e bisogna cercare di ridurre la dimensione dei dati (PCA)
	\subsection{Confronto con Algoritmi Visti}
	\begin{itemize}
		\item \textbf{vs Alberi di Decisione}: 
		\begin{itemize}
			\item k-NN cattura relazioni non lineari complesse, ma è più lento
			\item Alberi più efficienti ma con divisioni assiali
		\end{itemize}
		\item \textbf{vs SVM}: 
		\begin{itemize}
			\item SVM più efficienti con grandi dataset
			\item k-NN non richiede kernel per non-linearità
		\end{itemize}
		\item \textbf{vs Reti Neurali}:
		\begin{itemize}
			\item NN apprendono rappresentazioni, k-NN opera nello spazio originale
			\item NN più scalabili in alta dimensionalità
		\end{itemize}
	\end{itemize}
	\subsection{Scelta k}
	La scelta della k (parametro che indica quanti punti meno distanti dall'istanza di test
   bisogna considerare) è molto importante, e con un dataset molto sbilanciato può portare
   la scelta ad essere sempre verso un'unica classe evitando le altre meno presenti\\
	Fortunatamente, nei casi realistici i dataset difficilmente sono sbilanciati, e k viene
   scelta più piccola di $\sqrt{m}$, m = elementi dataset\\
	Infatti, una considerazione importante da fare è che la scelta di un k piccolo porta a
   basso bias ed alta varianza, invece un k grande porta ad alto bias e bassa varianza